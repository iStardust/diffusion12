{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\19941\\anaconda3\\envs\\gm\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from typing import Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "class ResidualConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels: int, out_channels: int, is_res: bool = False\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        standard ResNet style convolutional block\n",
    "        \"\"\"\n",
    "        self.same_channels = in_channels == out_channels\n",
    "        self.is_res = is_res\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.is_res:\n",
    "            x1 = self.conv1(x)\n",
    "            x2 = self.conv2(x1)\n",
    "            # this adds on correct residual in case channels have increased\n",
    "            if self.same_channels:\n",
    "                out = x + x2\n",
    "            else:\n",
    "                out = x1 + x2\n",
    "            return out / 1.414\n",
    "        else:\n",
    "            x1 = self.conv1(x)\n",
    "            x2 = self.conv2(x1)\n",
    "            return x2\n",
    "\n",
    "\n",
    "class UnetDown(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UnetDown, self).__init__()\n",
    "        \"\"\"\n",
    "        process and downscale the image feature maps\n",
    "        \"\"\"\n",
    "        layers = [ResidualConvBlock(in_channels, out_channels), nn.MaxPool2d(2)]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class UnetUp(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UnetUp, self).__init__()\n",
    "        \"\"\"\n",
    "        process and upscale the image feature maps\n",
    "        \"\"\"\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, 2, 2),\n",
    "            ResidualConvBlock(out_channels, out_channels),\n",
    "            ResidualConvBlock(out_channels, out_channels),\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        \n",
    "        x = torch.cat((x, skip), 1)\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EmbedFC(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim):\n",
    "        super(EmbedFC, self).__init__()\n",
    "        \"\"\"\n",
    "        generic one layer FC NN for embedding things  \n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        layers = [\n",
    "            nn.Linear(input_dim, emb_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class ContextUnet(nn.Module):\n",
    "    def __init__(self, in_channels, n_feat=256, n_classes=10):\n",
    "        super(ContextUnet, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.n_feat = n_feat\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\n",
    "\n",
    "        self.down1 = UnetDown(n_feat, n_feat)\n",
    "        self.down2 = UnetDown(n_feat, 2 * n_feat)\n",
    "\n",
    "        self.to_vec = nn.Sequential(nn.AvgPool2d(7), nn.GELU())\n",
    "\n",
    "        self.timeembed1 = EmbedFC(1, 2 * n_feat)\n",
    "        self.timeembed2 = EmbedFC(1, 1 * n_feat)\n",
    "        self.contextembed1 = EmbedFC(n_classes, 2 * n_feat)\n",
    "        self.contextembed2 = EmbedFC(n_classes, 1 * n_feat)\n",
    "\n",
    "        self.up0 = nn.Sequential(\n",
    "            # nn.ConvTranspose2d(6 * n_feat, 2 * n_feat, 7, 7), # when concat temb and cemb end up w 6*n_feat\n",
    "            nn.ConvTranspose2d(\n",
    "                2 * n_feat, 2 * n_feat, 7, 7\n",
    "            ),  # otherwise just have 2*n_feat\n",
    "            nn.GroupNorm(8, 2 * n_feat),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.up1 = UnetUp(4 * n_feat, n_feat)\n",
    "        self.up2 = UnetUp(2 * n_feat, n_feat)\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(2 * n_feat, n_feat, 3, 1, 1),\n",
    "            nn.GroupNorm(8, n_feat),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_feat, self.in_channels, 3, 1, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, c, t, context_mask):\n",
    "        # x is (noisy) image, c is context label, t is timestep,\n",
    "        # context_mask says which samples to block the context on\n",
    "\n",
    "        x = self.init_conv(x)\n",
    "        down1 = self.down1(x)\n",
    "        down2 = self.down2(down1)\n",
    "        hiddenvec = self.to_vec(down2)\n",
    "\n",
    "        # convert context to one hot embedding\n",
    "        c = nn.functional.one_hot(c, num_classes=self.n_classes).type(torch.float)\n",
    "\n",
    "        # mask out context if context_mask == 1\n",
    "        context_mask = context_mask[:, None]\n",
    "        context_mask = context_mask.repeat(1, self.n_classes)\n",
    "        context_mask = -1 * (1 - context_mask)  # need to flip 0 <-> 1\n",
    "        c = c * context_mask\n",
    "\n",
    "        # embed context, time step\n",
    "        cemb1 = self.contextembed1(c).view(-1, self.n_feat * 2, 1, 1)\n",
    "        temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1)\n",
    "        cemb2 = self.contextembed2(c).view(-1, self.n_feat, 1, 1)\n",
    "        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)\n",
    "\n",
    "        # could concatenate the context embedding here instead of adaGN\n",
    "        # hiddenvec = torch.cat((hiddenvec, temb1, cemb1), 1)\n",
    "\n",
    "        up1 = self.up0(hiddenvec)\n",
    "        # up2 = self.up1(up1, down2) # if want to avoid add and multiply embeddings\n",
    "        up2 = self.up1(cemb1 * up1 + temb1, down2)  # add and multiply embeddings\n",
    "        up3 = self.up2(cemb2 * up2 + temb2, down1)\n",
    "        out = self.out(torch.cat((up3, x), 1))\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "def ddpm_schedules(beta1, beta2, T):\n",
    "    \"\"\"\n",
    "    Returns pre-computed schedules for DDPM sampling, training process.\n",
    "    \"\"\"\n",
    "    assert beta1 < beta2 < 1.0, \"beta1 and beta2 must be in (0, 1)\"\n",
    "\n",
    "    beta_t = (beta2 - beta1) * torch.arange(0, T + 1, dtype=torch.float32) / T + beta1\n",
    "    sqrt_beta_t = torch.sqrt(beta_t)\n",
    "    alpha_t = 1 - beta_t\n",
    "    log_alpha_t = torch.log(alpha_t)\n",
    "    alphabar_t = torch.cumsum(log_alpha_t, dim=0).exp()\n",
    "\n",
    "    sqrtab = torch.sqrt(alphabar_t)\n",
    "    oneover_sqrta = 1 / torch.sqrt(alpha_t)\n",
    "\n",
    "    sqrtmab = torch.sqrt(1 - alphabar_t)\n",
    "    mab_over_sqrtmab_inv = (1 - alpha_t) / sqrtmab\n",
    "\n",
    "    return {\n",
    "        \"alpha_t\": alpha_t,  # \\alpha_t\n",
    "        \"oneover_sqrta\": oneover_sqrta,  # 1/\\sqrt{\\alpha_t}\n",
    "        \"sqrt_beta_t\": sqrt_beta_t,  # \\sqrt{\\beta_t}\n",
    "        \"alphabar_t\": alphabar_t,  # \\bar{\\alpha_t}\n",
    "        \"sqrtab\": sqrtab,  # \\sqrt{\\bar{\\alpha_t}}\n",
    "        \"sqrtmab\": sqrtmab,  # \\sqrt{1-\\bar{\\alpha_t}}\n",
    "        \"mab_over_sqrtmab\": mab_over_sqrtmab_inv,  # (1-\\alpha_t)/\\sqrt{1-\\bar{\\alpha_t}}\n",
    "    }\n",
    "\n",
    "\n",
    "class DDPM(nn.Module):\n",
    "    def __init__(self, nn_model, betas, n_T, device, drop_prob=0.1):\n",
    "        super(DDPM, self).__init__()\n",
    "        self.nn_model = nn_model.to(device)\n",
    "\n",
    "        # register_buffer allows accessing dictionary produced by ddpm_schedules\n",
    "        # e.g. can access self.sqrtab later\n",
    "        for k, v in ddpm_schedules(betas[0], betas[1], n_T).items():\n",
    "            self.register_buffer(k, v)\n",
    "\n",
    "        self.n_T = n_T\n",
    "        self.device = device\n",
    "        self.drop_prob = drop_prob\n",
    "        self.loss_mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        \"\"\"\n",
    "        this method is used in training, so samples t and noise randomly\n",
    "        \"\"\"\n",
    "\n",
    "        _ts = torch.randint(1, self.n_T + 1, (x.shape[0],)).to(\n",
    "            self.device\n",
    "        )  # t ~ Uniform(0, n_T)\n",
    "        noise = torch.randn_like(x)  # eps ~ N(0, 1)\n",
    "\n",
    "        x_t = (\n",
    "            self.sqrtab[_ts, None, None, None] * x\n",
    "            + self.sqrtmab[_ts, None, None, None] * noise\n",
    "        )  # This is the x_t, which is sqrt(alphabar) x_0 + sqrt(1-alphabar) * eps\n",
    "        # We should predict the \"error term\" from this x_t. Loss is what we return.\n",
    "\n",
    "        # dropout context with some probability\n",
    "        context_mask = torch.bernoulli(torch.zeros_like(c) + self.drop_prob).to(\n",
    "            self.device\n",
    "        )\n",
    "\n",
    "        # return MSE between added noise, and our predicted noise\n",
    "        return self.loss_mse(noise, self.nn_model(x_t, c, _ts / self.n_T, context_mask))\n",
    "\n",
    "    def sample(self, n_sample, size, device, guide_w=0.0):\n",
    "        # we follow the guidance sampling scheme described in 'Classifier-Free Diffusion Guidance'\n",
    "        # to make the fwd passes efficient, we concat two versions of the dataset,\n",
    "        # one with context_mask=0 and the other context_mask=1\n",
    "        # we then mix the outputs with the guidance scale, w\n",
    "        # where w>0 means more guidance\n",
    "\n",
    "        x_i = torch.randn(n_sample, *size).to(\n",
    "            device\n",
    "        )  # x_T ~ N(0, 1), sample initial noise\n",
    "        c_i = torch.arange(0, 12).to(\n",
    "            device\n",
    "        )  # context for us just cycles throught the mnist labels\n",
    "        c_i = c_i.repeat(int(n_sample / c_i.shape[0]))\n",
    "\n",
    "        # don't drop context at test time\n",
    "        context_mask = torch.zeros_like(c_i).to(device)\n",
    "\n",
    "        # double the batch\n",
    "        c_i = c_i.repeat(2)\n",
    "        context_mask = context_mask.repeat(2)\n",
    "        context_mask[n_sample:] = 1.0  # makes second half of batch context free\n",
    "\n",
    "        x_i_store = []  # keep track of generated steps in case want to plot something\n",
    "        print(self.n_T)\n",
    "        for i in range(self.n_T, 0, -1):\n",
    "            print(f\"sampling timestep {i}\", end=\"\\r\")\n",
    "            t_is = torch.tensor([i / self.n_T]).to(device)\n",
    "            t_is = t_is.repeat(n_sample, 1, 1, 1)\n",
    "\n",
    "            # double batch\n",
    "            x_i = x_i.repeat(2, 1, 1, 1)\n",
    "            t_is = t_is.repeat(2, 1, 1, 1)\n",
    "\n",
    "            z = torch.randn(n_sample, *size).to(device) if i > 1 else 0\n",
    "\n",
    "            # split predictions and compute weighting\n",
    "            eps = self.nn_model(x_i, c_i, t_is, context_mask)\n",
    "            eps1 = eps[:n_sample]\n",
    "            eps2 = eps[n_sample:]\n",
    "            eps = (1 + guide_w) * eps1 - guide_w * eps2\n",
    "            x_i = x_i[:n_sample]\n",
    "            x_i = (\n",
    "                self.oneover_sqrta[i] * (x_i - eps * self.mab_over_sqrtmab[i])\n",
    "                + self.sqrt_beta_t[i] * z\n",
    "            )\n",
    "            if i % 20 == 0 or i == self.n_T or i < 8:\n",
    "                x_i_store.append(x_i.detach().cpu().numpy())\n",
    "\n",
    "        x_i_store = np.array(x_i_store)\n",
    "        return x_i, x_i_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading airplane\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5001/5001 [00:00<00:00, 521706.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading bus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5001/5001 [00:00<00:00, 585979.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading car\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5001/5001 [00:00<00:00, 361569.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5001/5001 [00:00<00:00, 677991.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading guitar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5001/5001 [00:00<00:00, 662028.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading laptop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5001/5001 [00:00<00:00, 770486.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pizza\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5001/5001 [00:00<00:00, 348874.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sea turtle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5001/5001 [00:00<00:00, 717020.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading star\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5001/5001 [00:00<00:00, 684474.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading t-shirt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5001/5001 [00:00<00:00, 748624.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading The Eiffel Tower\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5001/5001 [00:00<00:00, 357965.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading yoga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5001/5001 [00:00<00:00, 682780.97it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "CLASSES = [\n",
    "    \"airplane\",\n",
    "    \"bus\",\n",
    "    \"car\",\n",
    "    \"fish\",\n",
    "    \"guitar\",\n",
    "    \"laptop\",\n",
    "    \"pizza\",\n",
    "    \"sea turtle\",\n",
    "    \"star\",\n",
    "    \"t-shirt\",\n",
    "    \"The Eiffel Tower\",\n",
    "    \"yoga\",\n",
    "]\n",
    "\n",
    "class sub12Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Each data is a tuple (image, label).\n",
    "    Image is of size [1, h, w], and label is an integer from 0 to 11, each representing a class.\n",
    "    \"\"\"\n",
    "\n",
    "    def _read_image(self, path):\n",
    "        \"\"\"\n",
    "        Given an image path, returns a grey image with shape [1, h, w].\n",
    "        \"\"\"\n",
    "        grey_image = Image.open(path).convert(\"L\")\n",
    "        grey_image = grey_image.resize((28, 28))\n",
    "        grey_np = np.array(grey_image)[None, :, :] / 255.0\n",
    "        # grey_np[grey_np < 0.99] = 0.0\n",
    "        grey_np = 1.0-grey_np\n",
    "        grey_np = grey_np*5.0\n",
    "        grey_np[grey_np>=0.99]=1.0\n",
    "        return grey_np.astype(np.float32)\n",
    "\n",
    "    def __init__(self, root_dir):\n",
    "        metadata = []\n",
    "        from tqdm import tqdm\n",
    "\n",
    "        for i, class_name in enumerate(CLASSES):\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            print(f\"Loading {class_name}\")\n",
    "            for file_name in tqdm(os.listdir(class_dir)):\n",
    "                metadata.append((os.path.join(class_dir, file_name), i))\n",
    "        self.metadata = metadata\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.metadata[idx]\n",
    "        return (torch.from_numpy(self._read_image(path)), label)\n",
    "    \n",
    "# hardcoding these here\n",
    "n_epoch = 50\n",
    "batch_size = 256\n",
    "n_T = 100  \n",
    "device = \"cuda:0\"\n",
    "n_classes = 12\n",
    "n_feat = 128  # 128 ok, 256 better (but slower)\n",
    "lrate = 2e-4\n",
    "save_model = False\n",
    "save_dir = \"./data/diffusion_outputs_12/\"\n",
    "ws_test = [0.0, 0.5, 2.0, 5.0]  # strength of generative guidance\n",
    "\n",
    "dataset = sub12Dataset(\"subclass12\")\n",
    "\n",
    "\n",
    "\n",
    "dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 1, 28, 28]) torch.Size([256])\n",
      "tensor([11,  7,  1,  7, 10,  9, 10,  7,  8,  3,  8,  4,  4,  3,  5,  7,  1,  7,\n",
      "        11,  8,  2,  3,  4,  8,  2,  1, 10,  1,  3,  0,  7,  9, 11,  4,  5,  3,\n",
      "         9, 11,  1,  7,  3,  3, 11,  8,  6,  3,  9,  3,  8,  1,  3,  2,  4,  1,\n",
      "         4,  5,  8,  9,  1,  5,  2,  6,  6, 11,  8,  0,  1, 10,  1,  8,  5, 11,\n",
      "         2,  2,  1,  5, 11,  3,  0,  9,  7,  8,  3,  2, 10,  4,  7,  0,  6,  4,\n",
      "         2,  7,  7,  6, 11,  4,  8,  0,  7,  5, 11,  9,  9, 11, 11,  4,  6,  9,\n",
      "         6,  4,  6,  2, 10, 10, 11, 11,  4,  4,  0, 11, 10,  2, 10,  9,  2,  9,\n",
      "         6,  8,  6,  8, 11,  0,  8,  2,  7,  1,  5,  0,  7,  0,  9,  9, 11,  7,\n",
      "         5, 10,  9,  2,  2,  2,  2,  6,  1, 11,  5,  8,  3,  7,  3,  0,  5,  0,\n",
      "         1, 10,  2,  7,  6,  1,  1,  1, 11,  6,  9,  1,  4,  5,  4,  6,  0,  2,\n",
      "         8,  9,  4,  9,  7,  5,  5, 11,  8,  6,  3,  4,  8,  4,  2,  4,  0,  8,\n",
      "         2,  8,  0,  3, 10,  3, 11,  9,  4,  8,  8,  0,  3,  7,  7, 10,  3,  1,\n",
      "         5, 11,  5,  2,  8,  5,  7,  9,  0,  1,  3, 10,  6,  8, 11,  6,  3,  1,\n",
      "         9,  3,  1, 10,  3,  4,  2,  2,  0,  3,  6,  9,  1,  4,  7,  7,  0, 10,\n",
      "         5,  9,  6,  8])\n",
      "tensor(0.) tensor(1.)\n",
      "[[0.         0.         0.6862745  0.8627451  0.47058824 0.4117647\n",
      "  0.64705884 1.         0.7254902  0.60784316 0.84313726 0.39215687\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.03921569 1.         0.84313726 0.01960784 0.03921569 0.\n",
      "  0.         0.27450982 0.21568628 0.03921569 0.11764706 1.\n",
      "  0.50980395 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.9411765  0.7254902  0.         0.         1.         0.05882353\n",
      "  0.         0.         0.25490198 0.9019608  0.         0.09803922\n",
      "  1.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [1.         0.         0.         0.         1.         0.\n",
      "  0.         0.         0.11764706 0.39215687 0.         0.\n",
      "  1.         0.03921569 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [1.         0.         0.         0.         0.05882353 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  1.         0.1764706  0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.84313726 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.92156863 0.09803922 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.9019608  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.21568628\n",
      "  1.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [1.         0.09803922 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.09803922 1.\n",
      "  0.3529412  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.5686275  1.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.1764706  1.         0.47058824\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.7647059  0.8627451  0.23529412 0.         0.\n",
      "  0.         0.09803922 0.5882353  0.7647059  0.25490198 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.23529412 0.6862745  1.         1.\n",
      "  1.         0.88235295 0.37254903 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.25490198 1.\n",
      "  0.03921569 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.21568628 0.9607843\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.11764706 1.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.09803922 1.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.09803922 1.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.11764706 1.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.98039216 1.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.1764706  1.         1.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.7647059  0.47058824 1.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.03921569 1.         0.03921569 1.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.5882353  0.7058824  0.         1.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.1764706  1.         0.01960784 0.         1.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.11764706 1.         0.8235294  0.09803922 1.\n",
      "  0.03921569 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.5686275  0.9411765  1.\n",
      "  0.03921569 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         1.\n",
      "  0.54901963 0.03921569 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.21568628\n",
      "  0.47058824 0.78431374 0.4117647  0.01960784 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.05882353 0.5882353  0.8235294  0.19607843 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for x,y in dataloader:\n",
    "    print(x.shape,y.shape)\n",
    "    print(y)\n",
    "    print(x.min(),x.max())\n",
    "\n",
    "    image = x.detach().cpu().numpy()[0]\n",
    "    image = np.reshape(image,(28,28))\n",
    "    print(image)\n",
    "    image = image*5.0\n",
    "    image[image>=0.99]=1.0\n",
    "    plt.imshow(image)\n",
    "\n",
    "    break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DDPM(\n",
       "  (nn_model): ContextUnet(\n",
       "    (init_conv): ResidualConvBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): GELU(approximate='none')\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (down1): UnetDown(\n",
       "      (model): Sequential(\n",
       "        (0): ResidualConvBlock(\n",
       "          (conv1): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): GELU(approximate='none')\n",
       "          )\n",
       "          (conv2): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "        (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "    (down2): UnetDown(\n",
       "      (model): Sequential(\n",
       "        (0): ResidualConvBlock(\n",
       "          (conv1): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): GELU(approximate='none')\n",
       "          )\n",
       "          (conv2): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "        (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "    (to_vec): Sequential(\n",
       "      (0): AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
       "      (1): GELU(approximate='none')\n",
       "    )\n",
       "    (timeembed1): EmbedFC(\n",
       "      (model): Sequential(\n",
       "        (0): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (timeembed2): EmbedFC(\n",
       "      (model): Sequential(\n",
       "        (0): Linear(in_features=1, out_features=128, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (contextembed1): EmbedFC(\n",
       "      (model): Sequential(\n",
       "        (0): Linear(in_features=12, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (contextembed2): EmbedFC(\n",
       "      (model): Sequential(\n",
       "        (0): Linear(in_features=12, out_features=128, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (up0): Sequential(\n",
       "      (0): ConvTranspose2d(256, 256, kernel_size=(7, 7), stride=(7, 7))\n",
       "      (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (up1): UnetUp(\n",
       "      (model): Sequential(\n",
       "        (0): ConvTranspose2d(512, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): ResidualConvBlock(\n",
       "          (conv1): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): GELU(approximate='none')\n",
       "          )\n",
       "          (conv2): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "        (2): ResidualConvBlock(\n",
       "          (conv1): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): GELU(approximate='none')\n",
       "          )\n",
       "          (conv2): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (up2): UnetUp(\n",
       "      (model): Sequential(\n",
       "        (0): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): ResidualConvBlock(\n",
       "          (conv1): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): GELU(approximate='none')\n",
       "          )\n",
       "          (conv2): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "        (2): ResidualConvBlock(\n",
       "          (conv1): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): GELU(approximate='none')\n",
       "          )\n",
       "          (conv2): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (out): Sequential(\n",
       "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(128, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (loss_mse): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "ddpm = DDPM(\n",
    "    nn_model=ContextUnet(in_channels=1, n_feat=n_feat, n_classes=n_classes),\n",
    "    betas=(1e-4, 0.02),\n",
    "    n_T=n_T,\n",
    "    device=device,\n",
    "    drop_prob=0.1,\n",
    ")\n",
    "ddpm.to(device)\n",
    "\n",
    "# optionally load a model\n",
    "# ddpm.load_state_dict(torch.load(\"./data/diffusion_outputs/ddpm_unet01_mnist_9.pth\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dataset = MNIST(\"./data\", train=True, download=True, transform=tf)\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2519:  25%|██▍       | 58/235 [01:29<04:31,  1.53s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(dataloader)\n\u001b[0;32m     11\u001b[0m loss_ema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\19941\\anaconda3\\envs\\gm\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\19941\\anaconda3\\envs\\gm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\19941\\anaconda3\\envs\\gm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\19941\\anaconda3\\envs\\gm\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\19941\\anaconda3\\envs\\gm\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[2], line 53\u001b[0m, in \u001b[0;36msub12Dataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m     52\u001b[0m     path, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata[idx]\n\u001b[1;32m---> 53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m), label)\n",
      "Cell \u001b[1;32mIn[2], line 28\u001b[0m, in \u001b[0;36msub12Dataset._read_image\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_image\u001b[39m(\u001b[38;5;28mself\u001b[39m, path):\n\u001b[0;32m     25\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124;03m    Given an image path, returns a grey image with shape [1, h, w].\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m     grey_image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m     grey_image \u001b[38;5;241m=\u001b[39m grey_image\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m))\n\u001b[0;32m     30\u001b[0m     grey_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(grey_image)[\u001b[38;5;28;01mNone\u001b[39;00m, :, :] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\19941\\anaconda3\\envs\\gm\\Lib\\site-packages\\PIL\\Image.py:3469\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3466\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[0;32m   3468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3469\u001b[0m     fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3470\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3471\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "optim = torch.optim.Adam(ddpm.parameters(), lr=lrate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optim,step_size=12,gamma=0.1)\n",
    "for ep in range(n_epoch):\n",
    "    print(f\"epoch {ep}\")\n",
    "    ddpm.train()\n",
    "\n",
    "    # linear lrate decay\n",
    "    optim.param_groups[0][\"lr\"] = lrate * (1 - ep / n_epoch)\n",
    "\n",
    "    pbar = tqdm(dataloader)\n",
    "    loss_ema = None\n",
    "    for x, c in pbar:\n",
    "        optim.zero_grad()\n",
    "        x = x.to(device)\n",
    "        c = c.to(device)\n",
    "        loss = ddpm(x, c)\n",
    "        loss.backward()\n",
    "        if loss_ema is None:\n",
    "            loss_ema = loss.item()\n",
    "        else:\n",
    "            loss_ema = 0.95 * loss_ema + 0.05 * loss.item()\n",
    "        pbar.set_description(f\"loss: {loss_ema:.4f}\")\n",
    "        optim.step()\n",
    "    scheduler.step()\n",
    "    # for eval, save an image of currently generated samples (top rows)\n",
    "    # followed by real images (bottom rows)\n",
    "    ddpm.eval()\n",
    "    with torch.no_grad():\n",
    "        n_sample = 4 * n_classes\n",
    "        for w_i, w in enumerate(ws_test):\n",
    "            x_gen, x_gen_store = ddpm.sample(\n",
    "                n_sample, (1, 28, 28), device, guide_w=w\n",
    "            )\n",
    "\n",
    "            # append some real images at bottom, order by class also\n",
    "            x_real = torch.Tensor(x_gen.shape).to(device)\n",
    "            for k in range(n_classes):\n",
    "                for j in range(int(n_sample / n_classes)):\n",
    "                    try:\n",
    "                        idx = torch.squeeze((c == k).nonzero())[j]\n",
    "                    except:\n",
    "                        idx = 0\n",
    "                    x_real[k + (j * n_classes)] = x[idx]\n",
    "\n",
    "            x_all = torch.cat([x_gen, x_real])\n",
    "            grid = make_grid(x_all * -1 + 1, nrow=12)\n",
    "            import os\n",
    "\n",
    "            if os.path.exists(save_dir) == False:\n",
    "                os.mkdir(save_dir)\n",
    "            save_image(grid, save_dir + f\"image_ep{ep}_w{w}.png\")\n",
    "            print(\"saved image at \" + save_dir + f\"image_ep{ep}_w{w}.png\")\n",
    "\n",
    "            if ep % 5 == 0 or ep == int(n_epoch - 1):\n",
    "                # create gif of images evolving over time, based on x_gen_store\n",
    "                fig, axs = plt.subplots(\n",
    "                    nrows=int(n_sample / n_classes),\n",
    "                    ncols=n_classes,\n",
    "                    sharex=True,\n",
    "                    sharey=True,\n",
    "                    figsize=(8, 3),\n",
    "                )\n",
    "\n",
    "                def animate_diff(i, x_gen_store):\n",
    "                    print(\n",
    "                        f\"gif animating frame {i} of {x_gen_store.shape[0]}\",\n",
    "                        end=\"\\r\",\n",
    "                    )\n",
    "                    plots = []\n",
    "                    for row in range(int(n_sample / n_classes)):\n",
    "                        for col in range(n_classes):\n",
    "                            axs[row, col].clear()\n",
    "                            axs[row, col].set_xticks([])\n",
    "                            axs[row, col].set_yticks([])\n",
    "                            # plots.append(axs[row, col].imshow(x_gen_store[i,(row*n_classes)+col,0],cmap='gray'))\n",
    "                            plots.append(\n",
    "                                axs[row, col].imshow(\n",
    "                                    -x_gen_store[i, (row * n_classes) + col, 0],\n",
    "                                    cmap=\"gray\",\n",
    "                                    vmin=(-x_gen_store[i]).min(),\n",
    "                                    vmax=(-x_gen_store[i]).max(),\n",
    "                                )\n",
    "                            )\n",
    "                    return plots\n",
    "\n",
    "                ani = FuncAnimation(\n",
    "                    fig,\n",
    "                    animate_diff,\n",
    "                    fargs=[x_gen_store],\n",
    "                    interval=200,\n",
    "                    blit=False,\n",
    "                    repeat=True,\n",
    "                    frames=x_gen_store.shape[0],\n",
    "                )\n",
    "                ani.save(\n",
    "                    save_dir + f\"gif_ep{ep}_w{w}.gif\",\n",
    "                    dpi=100,\n",
    "                    writer=PillowWriter(fps=5),\n",
    "                )\n",
    "                print(\"saved image at \" + save_dir + f\"gif_ep{ep}_w{w}.gif\")\n",
    "    # optionally save model\n",
    "    if ep % 5 == 0:\n",
    "        torch.save(ddpm.state_dict(), save_dir + f\"model_{ep}.pth\")\n",
    "        print(\"saved model at \" + save_dir + f\"model_{ep}.pth\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
